---
title: "Abgabe Gruppe C"
author: "Eric Beier, Lucas Oldenburg, Martin König, "
date: "30/01/2022"
output: html_document
---

## Bibliotheksimporte
```{r}
library(readxl)
library(ggplot2)
library(tidyr)
library(stringr)
library(lubridate)
library(mltools)
library(data.table)
library(mlbench)
library(tidyverse)
library(magrittr)

#svm
library(caret)
library(e1071) 

#tree
library(rpart)
library(rpart.plot)
library(ROCR)
library(h2o)
h2o.init()

```

## Datenimport onehot
```{r}

# Datenset nach One-Hot-Encoding
hot_df <- read_xlsx("20220110_dataPrepped_afterOneHot.xlsx")
hot_df <- subset(hot_df, 
                 select = -c(EmployeeChurned_0,
                                     `start_Cost center`,
                                     `start_Calendar Year/Month`,
                                     `LastActive_Calendar Year/Month`))

colnames(hot_df)[colnames(hot_df) == "EmployeeChurned_1"] <- "EmployeeChurned"

# Kategorische Variablen
factorColsOneHot <- c("Gender",
                "TemporaryPersonel",
                "Part Time Mark",
                "NonExempt",
                "start_NonExempt",
                "development to exempt",
                "Vertical_Horizontal_Developments_>= 3",
                "Vertical_Horizontal_Developments_0",
                "Vertical_Horizontal_Developments_1-2",
                "start_contract type_apprentice",
                "start_contract type_permanent staff",
                "start_contract type_temporary personnel",
                "start_Part time mark",
                "EmployeeChurned",
                "decrease_Category_Customer",
                "decrease_Category_GB Management",
                "decrease_Category_GBR/N",
                "decrease_Category_GTU GD",
                "decrease_Category_Human Resources GBS",
                "decrease_Category_Intellectual Property",
                "decrease_Category_IT Service CC",
                "decrease_Category_Other", #fehlt bei den anderen
                "decrease_Category_People",
                "decrease_Category_Planning & Development",
                "decrease_Category_Procurement Services - GP",
                "decrease_Category_RAA/A Competence",
                "decrease_Category_Safety",
                "decrease_Category_Supplier & Reporting",
                "Nationality_Classification_EU",
                "Nationality_Classification_German",
                "Nationality_Classification_Other",
                "Paybands_A",
                "Paybands_B",
                "Paybands_C",
                "Paybands_D",
                "Paybands_E",
                "Paybands_F",
                "Paybands_G",
                "Paybands_H")

# Umwandlung in kategorische Variablen
hot_df %<>% mutate_at(factorColsOneHot, factor)
```

## Datenimport und Vorbereitung
```{r}
# Rohdatenset ohne One-Hot-Encoding
df <- read_xlsx("20220110_dataPrepped_beforeOneHot.xlsx")

df <- subset(df, select = -c(`start_Cost center`,
                              `start_Calendar Year/Month`,
                              `LastActive_Calendar Year/Month`))
# Kategorische Variablen
factorCols <- c("Gender",
                "TemporaryPersonel",
                "Part Time Mark",
                "NonExempt",
                "start_NonExempt",
                "development to exempt",
                "Vertical_Horizontal_Developments",
                "start_contract type",
                "start_Part time mark",
                "EmployeeChurned",
                "decrease_Category",
                "Nationality_Classification",
                "Paybands")
df %<>% mutate_at(factorCols, factor)

```

## H2O Vorbereitung und Umwandlung der Daten
```{r}
dfH2O <- as.h2o(df)

#in H2o Test und Train erzeugen, da sonst keine validen Ergebnisse
splits <- h2o.splitFrame(data =  dfH2O, ratios = 0.75, seed = 1234)
trainH2O <- splits[[1]]
testH2O <- splits[[2]]

#h2o hot
hot_dfH2O <- as.h2o(hot_df)
splits1 <- h2o.splitFrame(data =  hot_dfH2O, ratios = 0.75, seed = 1234)
hot_train <- splits1[[1]]
hot_test <- splits1[[2]]


```

## Erstellen der Train und Test Datensets
```{r}
data_hot_test <- subset(hot_df, select = c("EmployeeChurned", "Age", "Month Of Service"))
df_hot <- data.frame(data_hot_test)
n <- nrow(df_hot)

train_indices <- sample(1:n, round(2/3 * n))
train <- df_hot[train_indices,]
test <- df_hot[-train_indices,]

# erstmal 2 Features auswählen 
features <- c("Age", "Month.Of.Service")
cols <- c(features, "EmployeeChurned")

```



## Einfacher Entscheidungsbaum mit rpart-Bibliothek
```{r}
rpart_control <- trainControl(
    method = "repeatedcv",
    number = 5,
    repeats = 10
)

#cp -> Complexity Parameter definiert die minimal zu erreichende Verbesserung je Ast
rpart_grid <- expand.grid(cp = c(0.015, 0.01, 0.005, 0.001, 
                                 0.0005, 0.0001, 0.00001))

rpart_fit <- train(EmployeeChurned ~ ., 
                       method = "rpart",
                       na.action = na.omit,
                       trControl = rpart_control,
                       tuneGrid = rpart_grid,
                       data = train)

best_cp <- rpart_fit$bestTune[1,1]
best_cp 
```

## Modell mit optimalen Parametern trainieren
```{r}
rpart <- rpart(EmployeeChurned ~., y = TRUE, x = TRUE,
                  data=train,
                      control= rpart.control(cp=best_cp))

rpart_prediction <- rpart.predict(rpart, test)
rpart_prediction <- data.frame(rpart_prediction)
test_rpart <- test
test_rpart$prob <- rpart_prediction$`X1`
test_rpart$EmployeeChurned <- as.numeric(as.character(test$EmployeeChurned))
test_rpart
```


## Allgemeine Modellgüte anhand der ROC-Kurve bestimmen
```{r}
# fpr_rocr <- perf_rocr@x.values[[1]]
pred_rocr_rpart <- prediction(test_rpart$prob, test_rpart$EmployeeChurned)
perf_rocr_rpart <- performance(pred_rocr_rpart, "tpr", "fpr" )

fpr_rocr_rpart <- perf_rocr_rpart@x.values[[1]]
tpr_rocr_rpart <- perf_rocr_rpart@y.values[[1]]
threshold_rocr_rpart <- perf_rocr_rpart@alpha.values[[1]] 

#AUC - Fläche unter der ROC-Kurve als Gütemerkmal
performance(pred_rocr_rpart, "auc")@y.values

# Darstellung der ROC-Kurve
plot(perf_rocr_rpart, col="blue")
abline(0,1)
abline(v=0.2, col="red")
```
Die Fläche unterhalb der ROC-Kurve ist größer als 0,5 und damit liefert 
das erstellte Modell eine genauere Vorhersage als bloßes Raten.

## Auswahl Threshold
```{r}
#Funktion zur Klassifizierung der Vorhersage
classify <- function(x, threshold){
  ifelse(x < threshold,0,1)
}

#Auswahl Threshold aus ROC-Kurve ("Ellenbogenpunkt")
threshold_predict <- 0.2
#Ein genauer "Ellenbogenpunkt" kann aus der Grafik nicht entnommen werden.
#Ziel sollte es sein FP-Klassifikationen zu vermeiden (kleinerer Threshold), aber dennoch 
#eine Balance zu finden, welche für eine Überklassifikation von Positiv-Klassifikation
#führt. Auch FN-Werte sorgen für Kosten im Unternehmen (siehe Kosten für Fehlklassifikation)

#Anwendung auf Testdatensatz
test_rpart$predict <-classify(test_rpart$prob, threshold_predict)
head(test_showPredict <- subset(test_rpart, select = c(prob,predict)),10)
```

## Modellgüte anhand der Konfusionsmatrix bestimmen
```{r}
confusionMatrix(factor(test_rpart$predict), factor(test$EmployeeChurned),
                positive = "1")
```
Aufgrund des niedrigen Schwellenwertes, ist die Genauigkeit (Accuracy) gering. 
Dafür weißt die balanzierte Genauigkeit (Balanced Accuracy) einen höheren Wert
als die klassische Genauigkeit auf. 
Es wird tendenziell erhöht auf eine Abwanderung der Mitarbeiter:innen klassifiziert.
Dabei ist die geringe Spezifizität auf die hohe Zahl an FN-klassifizierten Werten 
zurückzuführen. 
Im Anwendungsfall ist dies nicht von Nachteil, da besonders FP-Klassifikationen
vermieden werden sollten. Um dies aktiv in die Modellierung zu integrieren 
sollen die mit den Fehlklassifikationen verbundenen Kosten in die Betrachtung
mit einbezogen werden.

## Kosten für Fehlklassifikationen
```{r}
#Kosten aufgrund von Zeit für Manager:in und Mitarbeiter:in sowie zusätzliche Trainings- oder Gehaltskosten
cost_FP <- 500  
#Kosten für neues Recruiting (Suchauftrag, Recruitingaufwand wie Planung, Gespräche, Onboarding)
cost_FN <- 3000

matrix_overview_cost <- matrix(c(0,3000,500,0), nrow=2)
colnames(matrix_overview_cost) <- c("Mitarbeiter:in geht", "Mitarbeiter:in bleibt")
rownames(matrix_overview_cost) <- c("Vorhersage Mitarbeiter:in geht", "Vorhersage Mitarbeiter:in bleibt")
matrix_overview_cost
```

## Optimierung mit Kostenfunktion
```{r}
pred_class <- function(x, threshold){
  ifelse(x < threshold,0,1)
}

cost <- function(prob, actual,threshold2, cost_FN_func, cost_FP_func){
    predicted_automate2 <- pred_class(prob,threshold2)
    count_FN <- sum(predicted_automate2 == 0 & actual == "1")
    count_FP <- sum(predicted_automate2 == 1 & actual == "0")
    cost <- count_FN * cost_FN_func + count_FP * cost_FP_func
    return (cost)
}

threshold_input_rpart <- c(0.05, 0.1, 0.15, 0.16, 0.17, 0.2, 0.23, 0.24, 0.25,
                     0.3, 0.35, 0.45, 0.7) 

i <- 1
costs_rpart <- c()

while (i < length(threshold_input_rpart)+1) {
    costs_rpart[[(length(costs_rpart) +1)]] <- cost(test$prob, test$EmployeeChurned, 
                                         threshold_input_rpart[i], cost_FN, cost_FP)
    i <- i+1
}

cost_matrix_rpart <- matrix(c(threshold_input_rpart,costs_rpart),nrow=length(threshold_input_rpart))
colnames(cost_matrix_rpart) <- c("Threshold", "Kosten in Euro")
cost_matrix_rpart
```
Mit Betrachtung der Kosten liegt der optimale Threshold für das Model bei 0,17
und sollte für Vorhersagen eingesetzt werden. Es wurde der höchst mögliche 
Threshold bei den geringsten Kosten gewhählt, um ein balanzierteres Ergebnis
zu erreichen.

## Modellgüte mit optimiertem Threshold anhand der Konfusionsmatrix bestimmen
```{r}
threshold_rpart_opti <- 0.17

test_rpart$predict_rpart_opti <-classify(test_rpart$prob, threshold_rpart_opti)

confusionMatrix(factor(test_rpart$predict_rpart_opti), factor(test_rpart$EmployeeChurned),
                positive = "1")
```
Es werden fast alle Mitarbeiter:innen erkannt, die das Unternehmen verlassen.
Allerdings ist die FN-Klassifikation sehr hoch. Eine Spezifizität von circa
1/3 würde für einen hohen Aufwand bei Managern führen und nicht unbedingt für
deren Zuspruch sorgen.
Doch aufgrund der hohen Sensitivität kann eine noch gute balanzierte Genauigkeit
von dem Modell bei Anwendung auf die Testdaten erreicht werden.

## Visualisierung des Entscheidungsbaumes (Wichtigkeit der Features)
```{r}
printcp(rpart)
prp(rpart)
```
Den stärksten Einflussfaktor bildet der Funktionsbereich in dem die Mitarbeiter:in
zum letzten aktiven Zeitpunkt tätig war. GB Management, Human Resources GBS und
Safety sind dabei die genannten Faktoren. Dabei handelt es sich um erst kürzlich
aufgebaute Einheiten, was die kurze Zugehörigkeit der Mitarbeiter:innen und
geringe Abwanderung beschreibt. In zukünftigen Anpassungen des Modells auf neue
Daten sollte dies schrittweise an die übrigen Funktionsbereiche angeglichen
werden.
Die nächste Spaltung erfolgt durch das Merkmal Befristung. 
Auch die Dauer der Unternehmenszugehörigkeit sowie der Zeitpunkt seit der
letzten Entwicklung kommen zum Einsatz, während das Alter und die Gehaltsstufe
in den darauffolgenden Splits relevant werden.

Nicht zum Einsatz kommen: "Part Time Mark", "NonExempt", "start_NonExempt",
                "development to exempt", "decrease_Category",
                "Nationality_Classification", "Paybands"


## H2O Distributed Random Forest (DRF)
Im DRF werden eine Reihe an Bäumen erzeugt. Jeder dieser lernt basierend auf einen Teil
des Datensets (Spalten und Reihen). Für die finale Vorhersage wird dann der Durchschnitt 
aus den Vorhersagen der einzelnen Bäume berechnet. Das verringert die Varianz.
Um robuste Bäume zu erhalten werden darüber hinaus auf jedem Level je Baum 
zufällig die betrachteten Spalten (Features) gewählt. Die Anzahl der verwendeten
Spalten wird dabei durch die Wurzel aus der Gesamtanzahl der Spalten definiert.

## H2O Distributed Random Forest und Grid-Search (Tuning Hyperparameter)
```{r}
predictors_drf <- c(factorCols, "Month Of Service", "Age", "Months Since Last Development")
target_drf <- "EmployeeChurned"

#Hyperparameter
drf_params <- list( balance_classes = c(TRUE, FALSE),
                    ntrees = c(30, 50, 70),
                    max_depth = c(10, 15, 20, 25),
                    min_split_improvement = c(0.001, 0.0001, 0.00001, 0.000001))
#0.0000001
```
## Hyperparameter
balance classes - default False
ntrees - default 50
max_depth - default 20
Min_split_improvement - default 0,00001

## Hyperparameterwahl
```{r}
#search_criteria strategy “Cartesian”
h2oGrid_drf <- h2o.grid("drf", x = predictors_drf, y = target_drf,
            grid_id = "h2oGrid_drf",
            training_frame = trainH2O,
            hyper_params = drf_params,
            seed = 1234)

#Auswahl der optimalen Parameter anhand der AUC der einzelnen Modelle
h2oGridPerf_drf <- h2o.getGrid(grid_id = "h2oGrid_drf",
                           sort_by = "auc",
                           decreasing = TRUE)

# Güte im Trainingsset
summary(h2oGridPerf_drf)
best_drf <- h2o.getModel(h2oGridPerf_drf@model_ids[[1]])
h2o.performance(best_drf, trainH2O)
```


## Güte anhand Threshold basierend auf F1-Metrik
```{r}
#Anwendung auf Testdatenset
best_drf_perf <- h2o.performance(model = best_drf,
                                  newdata = testH2O)
best_drf_perf
```
Der Threshold wird automatisch auf die Metrik F1 optimiert.
Empfohlener Threshold bei Maximierung von F1 bei ca. 0,27.
Accuracy: ca. 73%
Balanced Accuracy: ca. 73%  (Spezifizität: ca. 0,63)

Ein deutlich niedriger Threshold (F1) als auf dem Trainingsdatenset ist zu erkennen
sowie eine hohe FP-Klassifizierungsrate.


## Auswahl Threshold anhand der ROC-Kurve
```{r}
# Anwendung auf Testset
predictBestGrid_drf <- h2o.predict(best_drf, newdata = testH2O)

# Umwandlung in generic dataframe 
drfPrediciton_df <- as.data.frame(predictBestGrid_drf)
testH2O_asdf_drf <- as.data.frame(testH2O)
testH2O_asdf_drf$prob <- drfPrediciton_df$p1
testH2O_asdf_drf

testH2O_asdf_drf$EmployeeChurned <- as.numeric(as.character(testH2O_asdf_drf$EmployeeChurned))
test

pred_rocr_drf <- prediction(testH2O_asdf_drf$prob, testH2O_asdf_drf$EmployeeChurned)
perf_rocr_drf <- performance(pred_rocr_drf, "tpr", "fpr" )

performance(pred_rocr_drf, "auc")@y.values
# Darstellung der ROC-Kurve
plot(perf_rocr_drf, col="blue")
abline(0,1)
abline(v=0.22, col="red")
```
Die ROC-Kurve verläuft deutlich oberhalb der Diagonalen. Bei Anwendung des 
Modells wird somit eine höhere Vorhersagequalität erreicht, als mit reinem
Raten.

## Auswahl Threshold aus ROC-Kurve ("Ellenbogenpunkt")
```{r}
threshold_drf <- 0.22
#Ein Ellenbogenpunkt ist ablesbar und wird tendenziell eher niedriger gewählt,
#um die Anzahl an FP-Klassifikationen zu minimieren.
```

## Modellgüte anhand der Konfusionsmatrix bestimmen
```{r}
testH2O_asdf_drf$predict <-classify(testH2O_asdf_drf$prob, threshold_drf)

confusionMatrix(factor(testH2O_asdf_drf$predict), factor(testH2O_asdf_drf$EmployeeChurned),
                positive = "1")
```
Im Vergleich zur Threshold-Wahl basierend auf der Maximierung des F1-Scores ist 
die Genauigkeit um fast 4 Prozentpunkte geringer, dafür ist die balanzierte
Genauigkeit nur geringfügig gesunken. Dafür ist die Spezifizität von 0,63 auf
0,88 stark angestiegen, was im Sinne des Use Cases wünschenswert ist, auch wenn
dafür die Sensitivität von ca. 0,83 auf 0,56 gesunken ist.

## Optimierung mit Kostenfunktion
```{r}
pred_class <- function(x, threshold){
  ifelse(x < threshold,0,1)
}

cost <- function(prob, actual,threshold2, cost_FN_func, cost_FP_func){
    predicted_automate2 <- pred_class(prob,threshold2)
    count_FN <- sum(predicted_automate2 == 0 & actual == "1")
    count_FP <- sum(predicted_automate2 == 1 & actual == "0")
    cost <- count_FN * cost_FN_func + count_FP * cost_FP_func
    return (cost)
}

threshold_input_drf <- c(0.05, 0.1, 0.14, 0.15, 0.16, 0.17, 0.18,
                         0.2, 0.23, 0.24, 0.25,
                         0.3, 0.35, 0.45, 0.7) 

i <- 1
costs_drf <- c()

while (i < length(threshold_input_drf)+1) {
    costs_drf[[(length(costs_drf) +1)]] <- cost(testH2O_asdf_drf$prob, 
                                        testH2O_asdf_drf$EmployeeChurned, 
                                        threshold_input_drf[i], cost_FN, cost_FP)
    i <- i+1
}

cost_drf_matrix <- matrix(c(threshold_input_drf,costs_drf),nrow=length(threshold_input_drf))
colnames(cost_drf_matrix) <- c("Threshold", "Kosten in Euro")
cost_drf_matrix
```
Mit Betrachtung der Kosten liegt der optimale Threshold für das Model bei 0,15.
Aufgrund der geringen Unterschiede wäre auch 0,17 als Threshold sinnvoll und
sollte für Vorhersagen eingesetzt werden.

## Modellgüte mit optimiertem Threshold anhand der Konfusionsmatrix bestimmen
```{r}
threshold_drf_opti <- 0.17

testH2O_asdf_drf$predict_opti <-classify(testH2O_asdf_drf$prob, threshold_drf_opti)

confusionMatrix(factor(testH2O_asdf_drf$predict_opti), factor(testH2O_asdf_drf$EmployeeChurned),
                positive = "1")

```
Verglichen mit vorher eingesetten Thresholds ist dies der niedrigste, da eine 
FP-Klassifikation stärker bestraft wird, als eine FN-Klassifikation. Das bestätigt
auch die hohe Sensitivität.
Dadurch sinkt die Genauigkeit sowie die balanzierte Genauigkeit, wobei letztere
größer ist und verglichen zum Ausgangswert (F1-Score-optimierter Threshold) nur 
um 0,04 niedriger ist. 
Die 311 FP-Klassifizierten Werte sollten im Rahmen des Use-Cases kritisch betrachtet
werden, da diese Daten zur Unterstützung von Managern gelten soll und mti diesem
Ergebnis viel Arbeit für diese Nutzergruppe entstehen würde. 

## Wichtigkeit der Features
```{r}
varimp_drf <- h2o.varimp(best_drf) 
varimp_drf
```
Wie im vorher eingesetzten Algorithmus (rpart) sind der Funktionsbereich,
die Dauer der Zugehörigkeit, die Dauer seit letzter Entwciklung, das Alter sowie
die Zuordnung in die Gehaltsbänder ausschlaggebend für die Vorhersage einer
Mitarbeiter:innen-Abwanderung. 
Auffällig ist, dass die Zuordnung in Befristung (TemporaryPersonel) eine weniger 
wichtige Rolle als im Modell basierend auf dem  rpart-Algorithmus einnimmt.


## Vergleich rpart und H2O Distributed Random Forest
```{r}
confusionMatrix(factor(test_rpart$predict_rpart), factor(test_rpart$EmployeeChurned),
                positive = "1")

confusionMatrix(factor(testH2O_asdf_drf$predict_opti), factor(testH2O_asdf_drf$EmployeeChurned),
                positive = "1")
```
Der Distributed Random Forest erzielt nicht nur allgemein eine größere AUC, auch
die Genauigkeit (inklusive balanzierter Genauigkeit) sind in den optimierten 
Modellen deutlich höher.
Gleicher Threshold nach Kostenfunktion(kann zufällig sein)




# Logistic Regression


## Predictors und response
```{r}
# zuerst die 6 Features auswählen, die die stärkste Abhängikeit nach den Trees haben
predictors_drf_strong <- c("decrease_Category",
                "Month Of Service",
                "Months Since Last Development",
                "Age",
                "Paybands",
                "Nationality_Classification")
# alle raw-Features, da H2O durch das setzen von alpha = 1 automatisch die besten Features auswählt
predictors <- c("Gender",
                "Part Time Mark",
                "TemporaryPersonel",
                "development to exempt",
                "Vertical_Horizontal_Developments",
                "start_contract type",
                "start_Part time mark",
                "decrease_Category",
                "Nationality_Classification",
                "Month Of Service",
                "Age",
                "Months Since Last Development",
                "NonExempt",
                "Paybands",
                "start_NonExempt")
# one-Hot
predictors_hot <- c("Gender",
                "TemporaryPersonel",
                "Part Time Mark",
                "NonExempt",
                "start_NonExempt",
                "development to exempt",
                "Vertical_Horizontal_Developments_>= 3",
                "Vertical_Horizontal_Developments_0",
                "Vertical_Horizontal_Developments_1-2",
                "start_contract type_apprentice",
                "start_contract type_permanent staff",
                "start_contract type_temporary personnel",
                "start_Part time mark",
                "decrease_Category_Customer",
                "decrease_Category_GB Management",
                "decrease_Category_GBR/N",
                "decrease_Category_GTU GD",
                "decrease_Category_Human Resources GBS",
                "decrease_Category_Intellectual Property",
                "decrease_Category_IT Service CC",
                "decrease_Category_Other",
                "decrease_Category_People",
                "decrease_Category_Planning & Development",
                "decrease_Category_Procurement Services - GP",
                "decrease_Category_RAA/A Competence",
                "decrease_Category_Safety",
                "decrease_Category_Supplier & Reporting",
                "Nationality_Classification_EU",
                "Nationality_Classification_German",
                "Nationality_Classification_Other",
                "Paybands_A",
                "Paybands_B",
                "Paybands_C",
                "Paybands_D",
                "Paybands_E",
                "Paybands_F",
                "Paybands_G",
                "Paybands_H",
                "Month Of Service",
                "Age",
                "Months Since Last Development")
response <- "EmployeeChurned"
```

## Model bauen
```{r}
glm_model <- h2o.glm(family = "binomial",
                     # x = predictors_drf_strong,
                     # x = predictors_hot,
                     x = predictors,
                     y = response,
                     training_frame = trainH2O,
                     # training_frame = hot_train,
                     alpha = 1,
                     lambda = 0.00001,
                     compute_p_values = FALSE)
summary(glm_model)
h2o.std_coef_plot(glm_model)
# AUC = 0.7636248 one hot
# AUC = 0.7636351 mit predictors
```
Es hat sich gezeigt, dass das Model ohne One-Hot Featueres mit allen Features am besten performt.

## Hyperparameterwahl und Tuning
https://docs.h2o.ai/h2o/latest-stable/h2o-docs/grid-search.html#supported-grid-search-hyperparameters
https://docs.h2o.ai/h2o/latest-stable/h2o-docs/grid-search.html#supported-grid-search-hyperparameters
https://docs.h2o.ai/h2o/latest-stable/h2o-docs/grid-search.html
```{r}
glm_params <- list( alpha = c(0.01, 0.25, 0.5, 0.75, 1),
                    lambda = c(1, 0.5, 0.1, 0.01, 0.001, 0.0001, 0.00001, 0)
                    # missing_values_handling = c("Skip", "MeanImputation", "PlugValues")
                    # standardize = c(TRUE, FALSE)
                    )
#search_criteria strategy “Cartesian”
h2oGrid <- h2o.grid("glm", x = predictors, y = response,
            grid_id = "h2oGrid",
            training_frame = trainH2O,
            hyper_params = glm_params,
            seed = 1234)

summary(h2oGrid, show_stack_traces = TRUE)

#Auswahl der optimalen Parameter anhand der AUC der einzelnen Modelle
h2oGridPerf <- h2o.getGrid(grid_id = "h2oGrid",
                           sort_by = "auc",
                           decreasing = TRUE)

# Güte im Trainingsset
summary(h2oGridPerf)
best_glm <- h2o.getModel(h2oGridPerf@model_ids[[1]])
h2o.performance(best_glm, trainH2O)
h2o.performance(best_glm, testH2O)
```
Es kann als Treshold abgelesen werden: max f1 =	0.292874

## Anwendung auf Testdaten und Auswahl Threshold
```{r}
# Anwendung auf Testset
pred <- h2o.predict(best_glm, newdata = testH2O)
Prediciton_df <- as.data.frame(pred)

test_asdf <- as.data.frame(testH2O)
test_asdf$prob <- Prediciton_df$p1

test_asdf$EmployeeChurned <- as.numeric(as.character(test_asdf$EmployeeChurned))

pred_rocr_glm <- prediction(test_asdf$prob, test_asdf$EmployeeChurned)
perf_rocr_glm <- performance(pred_rocr_glm, "tpr", "fpr" )

performance(pred_rocr_glm, "auc")@y.values
# Darstellung der ROC-Kurve mit Threshhold
plot(perf_rocr_glm, col="blue")
abline(0,1)
abline(v=0.29, col="green")
abline(v=0.292874, col="red")
```
Die ROC-Kurve verläuft deutlich oberhalb der Diagonalen. Bei Anwendung des 
Modells wird somit eine höhere Vorhersagequalität erreicht, als mit reinem
Raten. Die grüne Linie zeigt den selbst geschätzten bzw. gesehenen Treshold am Ellenbogenpunkt. 
Die rote Line zeigt den berechneten Treshold.

## Auswahl Threshold aus ROC-Kurve ("Ellenbogenpunkt")
```{r}
threshold_glm_gesehen <- 0.29
threshold_glm <- 0.292874
```

## Modellgüte anhand der Konfusionsmatrix bestimmen und Threshholds vergleichen
```{r}
classify <- function(x, threshold){
  ifelse(x < threshold,0,1)
}
test_asdf$predict1 <- classify(test_asdf$prob, threshold = threshold_glm_gesehen)
test_asdf$predict2 <- classify(test_asdf$prob, threshold = threshold_glm_gesehen)

confusionMatrix(factor(test_asdf$predict1), factor(test_asdf$EmployeeChurned),
                positive = "1")
confusionMatrix(factor(test_asdf$predict2), factor(test_asdf$EmployeeChurned),
                positive = "1")
```
Der gesehene Ellnbogenpunkt und der berechnete threshhold liefern identische Ergebnisse auf dem Testdatensatz.

## Optimierung mit Kostenfunktion
```{r}
pred_class <- function(x, threshold){
  ifelse(x < threshold,0,1)
}

cost <- function(prob, actual,threshold2, cost_FN_func, cost_FP_func){
    predicted_automate2 <- pred_class(prob,threshold2)
    count_FN <- sum(predicted_automate2 == 0 & actual == "1")
    count_FP <- sum(predicted_automate2 == 1 & actual == "0")
    cost <- count_FN * cost_FN_func + count_FP * cost_FP_func
    return (cost)
}

threshold_input_glm <- c(0.05, 0.1, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19,
                         0.2, 0.23, 0.24, 0.25, 0.29, 0.292874,
                         0.3, 0.35, 0.45, 0.7) 

i <- 1
costs_glm <- c()
cost_FN <- 3000
cost_FP <- 500

while (i < length(threshold_input_glm)+1) {
    costs_glm[[(length(costs_glm) +1)]] <- cost(test_asdf$prob, 
                                                test_asdf$EmployeeChurned, 
                                                threshold_input_glm[i], cost_FN, cost_FP)
    i <- i+1
}

cost_glm_matrix <- matrix(c(threshold_input_glm,costs_glm),nrow=length(threshold_input_glm))
colnames(cost_glm_matrix) <- c("Threshold", "Kosten in Euro")
cost_glm_matrix

```
Mit Betrachtung der Kosten liegt der optimale Threshold für das Model bei 0,18.

## Modellgüte mit optimiertem Threshold anhand der Konfusionsmatrix bestimmen
```{r}
threshold_glm_opti <- 0.18
test_asdf$predict_opti <-classify(test_asdf$prob, threshold_glm_opti)
confusionMatrix(factor(test_asdf$predict_opti), factor(test_asdf$EmployeeChurned),
                positive = "1")
```
Verglichen mit vorher eingesetzten Thresholds ist dies der niedrigste, da eine 
FP-Klassifikation stärker bestraft wird, als eine FN-Klassifikation. Das bestätigt
auch die hohe Sensitivität.
Dadurch sinkt die Genauigkeit sowie die balanzierte Genauigkeit, wobei letztere
größer ist und verglichen zum Ausgangswert (F1-Score-optimierter Threshold) nur 
um 0,04 niedriger ist. 
Die 383 FP-Klassifizierten Werte sollten im Rahmen des Use-Cases kritisch betrachtet
werden, da diese Daten zur Unterstützung von Managern gelten soll und mti diesem
Ergebnis viel Arbeit für diese Nutzergruppe entstehen würde.

## Wichtigkeit der Features
```{r}
varimp_glm <- h2o.varimp(best_glm)
varimp_glm
```

# Support Vecto Machine
## Datenimport und Vorbereitung
```{r}
df_svm <- read_xlsx("20220110_dataPrepped_beforeOneHot.xlsx")

# Kategorische Variablen umwandeln
factorCols <- c("EmployeeChurned",
                "decrease_Category",
                "Nationality_Classification",
                "Paybands")

df_svm %<>% mutate_at(factorCols, factor)
# Auswählen der zu nutzenden Spalten auf Grund der Ergebnisse von Random Forrest, die Nutzung aller Features ist mit einem sehr hohen Rechenaufwand verbunden, der nicht geleistet werden konnte.
df_svm <- subset(df_svm, select = c(EmployeeChurned, decrease_Category, `Month Of Service`, `Months Since Last Development`, Age, Paybands, Nationality_Classification))

# Ansehen der Daten
plot(df_svm, col=df_svm$EmployeeChurned)
```

## Erstellen der Train und Test Datensets
```{r}
data_test <- subset(hot_df, select = c("EmployeeChurned", "Age", "Month Of Service"))
df_test <- data.frame(data_test)
n <- nrow(df_test)

train_indices <- sample(1:n, round(2/3 * n))
train <- df_test[train_indices,]
test <- df_test[-train_indices,]

# erstmal 2 Features auswählen 
features <- c("Age", "Month.Of.Service")
cols <- c(features, "EmployeeChurned")

```


## SVM
```{r}

# erstmal 2 Features auswählen 
features <- c("Age", "Month.Of.Service")
cols <- c(features, "EmployeeChurned")

# fit an SVM with linear kernel
svmfit <- svm(EmployeeChurned ~ ., data = train[,cols], kernel = "linear", 
              cost = 1, scale = TRUE)

# information about the model including the support vectors
summary(svmfit)

# plot it including the separation lines (hyperplanes)
plot(svmfit, train[,cols])

# predict on the test data set
prediction <- predict(svmfit, test, type = "class")

# confusion matrix with quality measures
confusionMatrix(prediction, test[,"EmployeeChurned"])

```
Wir erkennen sowohl an der Confusions Matrix, als auch am Plot, dass die svm Methode hier nicht zum Erfolg führt, da alle Predictions 0 sind. Dies wird an der Art unserer Daten liegen und wie das Data Frame nach dem One-Hot-Encoding aufgebaut ist. Daher haben wir uns entschieden auf das H2O-Paket umzusteigen, welches selbst die Daten richtig aufbereitet und wir die Rohdaten nutzen konnten.



Wir haben bewusst nicht alle Features betrachtet, da nicht alle Merkmale einen signifikanten Einfluss haben. Mehr wollten wir auf den Ergebnissen der anderen Methoden aufbauen und ermitteln, ob wir eine noch bessere Vorhersage mit der Support-Vector-Machine erreichen können. Des Weiteren stand uns nicht genügend Rechenkraft zur Seite beim Tuning um alle Features zu berücksichtigen. 


## H2O Vorbereitung und Umwandlung der Daten
```{r}
df_svmH2O <- as.h2o(df_svm)

#in H2o Test und Train erzeugen, da sonst keine validen Ergebnisse
splits <- h2o.splitFrame(data =  df_svmH2O, ratios = 0.75, seed = 1234)
trainH2O_svm <- splits[[1]]
testH2O_svm <- splits[[2]]

target <- "EmployeeChurned"
# erstmal 2 Features auswählen die die stärkste Abhängikeit nach den Trees haben
predictors <- c( "decrease_Category", "Month Of Service")

```

## SVM mit H2O
```{r}
#https://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/svm.html
#SVM durchführen 
svm_model <- h2o.psvm(
  rank_ratio = 0.1,
  y ="EmployeeChurned",
  training_frame = trainH2O_svm, 
  disable_training_metrics = FALSE
  )

#svm auf Testdatensatz testen
svm_model_h20 <- h2o.getModel(svm_model@model_id)
perf_svm <- h2o.performance(svm_model_h20, testH2O_svm)
perf_svm
```
Die H20.psvm Methode führt eine svm aus und legt zusätzlich alle Informationen in das Model wie zum Beispiel die Confusions Matrix. Darüber hinaus wurde das Model auf den Testdatensatz angewendet. Das Ergebnis ist eine durchschnittlicher Error von ca. 40%. Dies ist wesentlich weniger als 50% und damit besser als raten (50%). Jedoch ist es noch weit entfernt von einer Vorhersage, die wir uns wünschen. 
Wir hätten gern wie beim DRF eine Optimierung der Parameter "Gamma" und "Cost" vorgenommen, nur leider ist dies in der H2O Bibliothek nicht möglich. Dies haben wir daher mit der e1071 Bibliothek realisiert, da wir rausgefunden haben, dass diese wieder anwendbar ist, nachdem umwandeln der der Daten von H20 und wieder zurück in ein "normales" DataFrame.


## SVM Tune Funktion mit e1071
Folgend verwenden wir die Tune Funktion um das beste "Gamma" und "Cost" zu ermitteln für eine optimale Vorhersage. 
```{r}
# Umwandeln der H20 DataFrames in "normale" DataFrames, um das Tuning der e1071 Methode ausführen können.
tunedf_svm <- as.data.frame(df_svmH2O)
trainDF_svm <- as.data.frame(trainH2O_svm)
testDF_svm <- as.data.frame(testH2O_svm)

#Tuning der Parameter -> hier hätten wir gern eine größere Bandbreite an gamma und cost Variablen durchlaufen lassen, wurde aber von unserer Rechenleistung nicht gewährleistet. Daher haben wir in einzelnen Schritten uns den jetztigen WErten angenähert um so einen möglichst optimalen Wert zu finden. Auch ist zu ebachten, dass wir keinen Gamma-Wert unter 0.01 nutzen konnten, da diese nicht mehr von den folgenden Methoden unterstützt werden.

tuneSVM <- function(svm_type) {

     tuned_svm <- tune(svm, EmployeeChurned ~ ., 
              data = trainDF_svm, 
              kernel = svm_type, 
              scale = TRUE, 
              ranges = list(cost = c(0.05, 0.075),  gamma = c(0.01, 0.02,0.25)), 
              tunecontrol = tune.control(cross=5)
              )
    tuned_svm
    
    # train on the entire data set using best value for cost
    best_cost <- as.numeric(tuned_svm$best.parameters[1])
    print(paste0("Cost: ", best_cost))
    
    best_gamma <- as.numeric(tuned_svm$best.parameters[2])
    print(paste0("Gamma: ", best_gamma))
        
    svmfit_best <- svm(EmployeeChurned ~ ., 
                  data = trainDF_svm, 
                  kernel = svm_type, 
                  cost = best_cost, 
                  gamma = best_gamma, 
                  scale = TRUE)
  return(svmfit_best)
}

```

## Linear SVM
```{r}
linear_svm <- tuneSVM("linear")

#plot(linear_svm, trainDF_svm[,cols])
#Leider ist die Plotfunktion nicht mehr möglich nach dem umwandeln der Daten in H20 und wieder zurück

#predict
prediction_linear <- predict(linear_svm, testDF_svm, type = "class")
    
# confusion matrix with quality measures
confusionMatrix(prediction_linear, testDF_svm[, "EmployeeChurned"])
```

## Radial SVM
```{r}
radial_svm <- tuneSVM("radial")

#predict
prediction_radial <- predict(radial_svm, testDF_svm, type = "class")
    
# confusion matrix with quality measures
confusionMatrix(prediction_radial, testDF_svm[, "EmployeeChurned"])
```

## SVM mit besten Parametern
Für H2O nutzen wir die ermittelten Gamma Werte von Linear, da diese ein höhere Accuracy haben.
```{r}
#https://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/svm.html
#SVM durchführen 
svm_model <- h2o.psvm(
  hyper_param = 0.075,
  gamma = 0.01,
  rank_ratio = 0.1,
  y ="EmployeeChurned",
  training_frame = trainH2O_svm, 
  disable_training_metrics = FALSE
  )

# mehr Infos über Model und support Vektoren
summary(svm_model)            

#svm auf Testdatensatz testen
best_svm_model_h20 <- h2o.getModel(svm_model@model_id)
best_perf_svm <- h2o.performance(best_svm_model_h20, testH2O_svm)
best_perf_svm
```

## Auswahl Threshold anhand ROC Kurve
```{r}
# Anwendung auf Testset
#predict
pred <- h2o.predict(best_svm_model_h20, newdata = testH2O_svm)

prediciton_DF_svm <- as.data.frame(pred)

asdf_svm <- testDF_svm
asdf_svm$prob <- prediciton_DF_svm$p1

asdf_svm$EmployeeChurned <- as.numeric(as.character(asdf_svm$EmployeeChurned))

pred_rocr_svm <- prediction(asdf_svm$prob, asdf_svm$EmployeeChurned)
perf_rocr_svm <- performance(pred_rocr_svm, "tpr", "fpr" )

performance(pred_rocr_svm, "auc")@y.values
# Darstellung der ROC-Kurve
plot(perf_rocr_svm, col="blue")
abline(0,1)

```
Die Fläche unterhalb der ROC-Kurve ist mit 0.6 größer als 0,5 und damit liefert 
das erstellte Modell eine genauere Vorhersage als bloßes Raten.


## Auswahl Threshold
Die Auswahl eines Thresholds ist bei einer SVM nicht sinnvoll, da das Ergebnis immer 1 oder 0 ist, d.h. entweder es ist auf der einen Seite der Linie oder der anderen. Daher ist der Threshold immer 1. Dies führt ebenfalls dazu, dass die Kostenfunktion nicht anwendbar ist, da der Threshold immer 1 ist, verändert sich das Ergebnis bei der Kostenfunktion ebenfalls nicht.

## Zusammenfassung SVM
Es wurden diverse Möglickeiten genutzt die beste Vorhersage durch das SVM-Modell zu bestimmen. Zu erst wurde das klassische Verfahren durch die e1071-Bibliothek angewendet. Auf Grund der Datenstruktur die vorliegt konnten keine Statistisch sinnvollen Vorhersagen gemacht werde, da diese alle auf "0" gesetzt wurden. Somit wurden keine unterschiedlicen Bereiche entdeckt. 
Aus diesem Grund wurde mit der H2O-Bibliothek ein neuer Ansatz gestartet, da hier die DataFrames in eigene h20-DataFrames umgewandelt werden. Dies war jedoch nur auf den Rohdaten anwendbar und nicht mehr auf den One-Hot-Encodeten Daten. Hierbei wurden nicht alle Daten eingelesen, sondern nur diese, die durch das RandomForest-Modell den stärksten Einfluss haben, da sonst die Rechenleistung beim Tuning nicht ausreicht. 
Die Ergebnisse der H2O Vorhersage lagen bei einem Error von ca. 40%.
Darauf hin wurde getestet, ob die SVM-Methode von e1071 mit dem H2O-DataFram funktioniert, da dort die Daten wieder abgewandelt wurden. Dies hat funktioniert und dadurch konnte ein Tuning des "Gamma" und "Cost" Wertes vorgenommen werden. Dies führte zu einer Genauigkeit von: 60% für die Radial-Methode und 67% bei der Linearen-Methode. 
Zur Vergelichbarkeit wurde eine ROC-Kurve konstuiert. Dabei war es leider nicht möglich die Modelle der e1071-Bibliothek zu nutzen, weshalb noch einmal die H2O-SVM methode mit den optimierten Parametern der e1071-tune-Funktion durchgeführt wurde. Dies führte zu einer Fehleranfälligkeit von 37% jedoch bei der uns wichtigen False-Negative Rate einen Error von 28%. Dies ist nochmals eine deutliche Verbessung zum Anfang und somit das beste Ergbnis des SVM-Modells.

# Vergleich aller Modelle

## Vergleich der Algorithmen anhand der einzelnen ROC-Kurven
```{r}
# pred_rocr_drf <- prediction(raw_testH2O_asdf$prob, raw_testH2O_asdf$EmployeeChurned)
# perf_rocr_drf <- performance(pred_rocr_drf, "tpr", "fpr" )
# pred_rocr <- prediction(raw_test$prob, raw_test$EmployeeChurned)
# perf_rocr <- performance(pred_rocr, "tpr", "fpr" )


# Darstellung der ROC-Kurve
plot(perf_rocr_rpart, col="red")
plot(perf_rocr_drf, col="blue", add=TRUE)
plot(perf_rocr_glm, col="purple", add=TRUE)
plot(perf_rocr_svm, col="green", add=TRUE)
abline(0,1)
legend(x = "bottomright",
       legend = c("Einfacher Baum (rpart)", "Distributed Random Forest (H2O)","Logistic Regression", "Support-Vector-Machine"),
       lty = c(1, 1, 1, 1),
       col = c("red", "blue", "purple", "green"))
title(main = "Vergleich der Algorithmen anhand der einzelnen ROC-Kurven")
```




## Vergleich anhand der Konfusionsmatrizen

Zum Vergleich aller Modelle, betrachten wir die Eintrittswahrscheinlichkeit von False-Positive Klassifikation unter allen False-Predictions, d.h. die Wahrscheinlichkeit, dass vorrausgesagt wird der Mitarbeitende bleibt im Unternehmen ist falsch. Des Weiteren betrachten wir die allgemeine Acurracy. Dabei betrachten wir lediglich die besten Vorhersagen je Modell.

```{r}
false_positive <- c(10, 7, 29)
acurracy <- c(65,59, 62)
auswertung <- matrix(c(false_positive, acurracy), nrow=3)
colnames(auswertung) <- c("False-Positve-Rate", "Acurracy in %")
rownames(auswertung) <- c("Distributed Random Forrest", "Logistic Regression", "Support-Vector Machine")
print(auswertung)

```

Die Matrix zeigt die zusammengefassten und berechneten Werte der besten Konfusionsmatrizen der verschiedenen Modelle. Klar erkennbar ist, dass die Anwendung des Support-Vector Machine Modells eine vergleichweise gute Genauigkeit vorweist wie die anderen. Jedoch schneidet die False-Positive-Rate wesentlich schlechter als die beiden anderen ab, was im Kontext des Anwendungsfalls zu höheren Kosten führen würde. Dies liegt daran, dass das Modell nicht extra daruaf trainiert werden konnte, da der Threshold wie oben beschrieben immer "1" ist. 
Daher wird jetzt der Unterschied der beiden anderen Modelle betrachtet. Dafür werden Kostenfunktion betrachtet.

Kosten für DRF: 240.000
Kosten für GLM: 233.500


Nach dem aktuellen Modell und den aktuell berechneten Kosten ist das GLM-Modell wirtschaftlich besser.








