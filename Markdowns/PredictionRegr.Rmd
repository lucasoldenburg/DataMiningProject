---
title: "Prediction Regression"
output: html_document
---

## Employee Churn
# Prediction LogRegression
```{r}
library(readxl)
library(ggplot2)
library(tidyr)
library(stringr)
library(lubridate)
library(mltools)
library(data.table)
library(mlbench)
library(tidyverse)
library(magrittr)
library(caret)

library(rpart)
library(rpart.plot)
library(ROCR)
library(h2o)
h2o.init()

```

## Datenimport und -vorbereitung
```{r}
# Rohdatenset ohne One-Hot-Encoding
raw_df <- read_xlsx("20220110_dataPrepped_beforeOneHot.xlsx")

raw_df <- subset(raw_df, 
                  select = -c(`start_Cost center`,
                              `start_Calendar Year/Month`,
                              `LastActive_Calendar Year/Month`))
# Kategorische Variablen
factorCols <- c("Gender",
                "TemporaryPersonel",
                "Part Time Mark",
                "NonExempt",
                "start_NonExempt",
                "development to exempt",
                "Vertical_Horizontal_Developments",
                "start_contract type",
                "start_Part time mark",
                "EmployeeChurned",
                "decrease_Category",
                "Nationality_Classification",
                "Paybands")
raw_df %<>% mutate_at(factorCols, factor)

# Datenset nach One-Hot-Encoding
hot_df <- read_xlsx("20220110_dataPrepped_afterOneHot.xlsx")
hot_df <- subset(hot_df, 
                 select = -c(EmployeeChurned_0,
                                     `start_Cost center`,
                                     `start_Calendar Year/Month`,
                                     `LastActive_Calendar Year/Month`))

colnames(hot_df)[colnames(hot_df) == "EmployeeChurned_1"] <- "EmployeeChurned"

# Kategorische Variablen
factorColsOneHot <- c("Gender",
                "TemporaryPersonel",
                "Part Time Mark",
                "NonExempt",
                "start_NonExempt",
                "development to exempt",
                "Vertical_Horizontal_Developments_>= 3",
                "Vertical_Horizontal_Developments_0",
                "Vertical_Horizontal_Developments_1-2",
                "start_contract type_apprentice",
                "start_contract type_permanent staff",
                "start_contract type_temporary personnel",
                "start_Part time mark",
                "EmployeeChurned",
                "decrease_Category_Customer",
                "decrease_Category_GB Management",
                "decrease_Category_GBR/N",
                "decrease_Category_GTU GD",
                "decrease_Category_Human Resources GBS",
                "decrease_Category_Intellectual Property",
                "decrease_Category_IT Service CC",
                "decrease_Category_Other", #fehlt bei den anderen
                "decrease_Category_People",
                "decrease_Category_Planning & Development",
                "decrease_Category_Procurement Services - GP",
                "decrease_Category_RAA/A Competence",
                "decrease_Category_Safety",
                "decrease_Category_Supplier & Reporting",
                "Nationality_Classification_EU",
                "Nationality_Classification_German",
                "Nationality_Classification_Other",
                "Paybands_A",
                "Paybands_B",
                "Paybands_C",
                "Paybands_D",
                "Paybands_E",
                "Paybands_F",
                "Paybands_G",
                "Paybands_H")

# Umwandlung in kategorische Variablen
hot_df %<>% mutate_at(factorColsOneHot, factor)

#Indexspalte
#hot_df$index <- 1:nrow(hot_df)
```

## H2O Vorbereitung und Umwandlung der Daten, Erstellen der Train und Test Datensets
```{r}
raw_df <- as.h2o(raw_df)
splits <- h2o.splitFrame(data =  raw_df, ratios = 0.75, seed = 1234)
raw_train <- splits[[1]]
raw_test <- splits[[2]]

hot_df <- as.h2o(hot_df)
splits1 <- h2o.splitFrame(data =  hot_df, ratios = 0.75, seed = 1234)
hot_train <- splits1[[1]]
hot_test <- splits1[[2]]
```

## Predictors und response
```{r}
# zuerst die 6 Features auswählen, die die stärkste Abhängikeit nach den Trees haben
predictors_drf <- c("decrease_Category",
                "Month Of Service",
                "Months Since Last Development",
                "Age",
                "Paybands",
                "Nationality_Classification")
# alle raw-Features, da H2O durch das setzen von alpha = 1 automatisch die besten Features auswählt
predictors <- c("Gender",
                "Part Time Mark",
                "TemporaryPersonel",
                "development to exempt",
                "Vertical_Horizontal_Developments",
                "start_contract type",
                "start_Part time mark",
                "decrease_Category",
                "Nationality_Classification",
                "Month Of Service",
                "Age",
                "Months Since Last Development",
                "NonExempt",
                "Paybands",
                "start_NonExempt")
# one-Hot
predictors_hot <- c("Gender",
                "TemporaryPersonel",
                "Part Time Mark",
                "NonExempt",
                "start_NonExempt",
                "development to exempt",
                "Vertical_Horizontal_Developments_>= 3",
                "Vertical_Horizontal_Developments_0",
                "Vertical_Horizontal_Developments_1-2",
                "start_contract type_apprentice",
                "start_contract type_permanent staff",
                "start_contract type_temporary personnel",
                "start_Part time mark",
                "decrease_Category_Customer",
                "decrease_Category_GB Management",
                "decrease_Category_GBR/N",
                "decrease_Category_GTU GD",
                "decrease_Category_Human Resources GBS",
                "decrease_Category_Intellectual Property",
                "decrease_Category_IT Service CC",
                "decrease_Category_Other",
                "decrease_Category_People",
                "decrease_Category_Planning & Development",
                "decrease_Category_Procurement Services - GP",
                "decrease_Category_RAA/A Competence",
                "decrease_Category_Safety",
                "decrease_Category_Supplier & Reporting",
                "Nationality_Classification_EU",
                "Nationality_Classification_German",
                "Nationality_Classification_Other",
                "Paybands_A",
                "Paybands_B",
                "Paybands_C",
                "Paybands_D",
                "Paybands_E",
                "Paybands_F",
                "Paybands_G",
                "Paybands_H",
                "Month Of Service",
                "Age",
                "Months Since Last Development")
response <- "EmployeeChurned"
```

## Model bauen
```{r}
glm_model <- h2o.glm(family = "binomial",
                     # x = predictors_drf,
                     # x = predictors_hot,
                     x = predictors,
                     y = response,
                     training_frame = raw_train,
                     # training_frame = hot_train,
                     alpha = 1,
                     lambda = 0.00001,
                     compute_p_values = FALSE)
summary(glm_model)
h2o.std_coef_plot(glm_model)
# AUC = 0.7636248 one hot
# AUC = 0.7636351 mit predictors
```
Es hat sich gezeigt, dass das Model ohne One-Hot Featueres mit allen Features am besten performt.

## Hyperparameterwahl und Tuning
https://docs.h2o.ai/h2o/latest-stable/h2o-docs/grid-search.html#supported-grid-search-hyperparameters
https://docs.h2o.ai/h2o/latest-stable/h2o-docs/grid-search.html#supported-grid-search-hyperparameters
https://docs.h2o.ai/h2o/latest-stable/h2o-docs/grid-search.html
```{r}
glm_params <- list( alpha = c(0.01, 0.25, 0.5, 0.75, 1),
                    lambda = c(1, 0.5, 0.1, 0.01, 0.001, 0.0001, 0.00001, 0)
                    # missing_values_handling = c("Skip", "MeanImputation", "PlugValues")
                    # standardize = c(TRUE, FALSE)
                    )
#search_criteria strategy “Cartesian”
h2oGrid <- h2o.grid("glm", x = predictors, y = response,
            grid_id = "h2oGrid",
            training_frame = raw_train,
            hyper_params = glm_params,
            seed = 1234)

summary(h2oGrid, show_stack_traces = TRUE)

#Auswahl der optimalen Parameter anhand der AUC der einzelnen Modelle
h2oGridPerf <- h2o.getGrid(grid_id = "h2oGrid",
                           sort_by = "auc",
                           decreasing = TRUE)

# Güte im Trainingsset
summary(h2oGridPerf)
best_glm <- h2o.getModel(h2oGridPerf@model_ids[[1]])
h2o.performance(best_glm, raw_train)
h2o.performance(best_glm, raw_test)
```
Es kann als Treshold abgelesen werden: max f1 =	0.292874

## Anwendung auf Testdaten und Auswahl Threshold
```{r}
# Anwendung auf Testset
pred <- h2o.predict(best_glm, newdata = raw_test)
Prediciton_df <- as.data.frame(pred)

test_asdf <- as.data.frame(raw_test)
test_asdf$prob <- Prediciton_df$p1

test_asdf$EmployeeChurned <- as.numeric(as.character(test_asdf$EmployeeChurned))

pred_rocr <- prediction(test_asdf$prob, test_asdf$EmployeeChurned)
perf_rocr <- performance(pred_rocr, "tpr", "fpr" )

performance(pred_rocr, "auc")@y.values
# Darstellung der ROC-Kurve mit Threshhold
plot(perf_rocr, col="blue")
abline(0,1)
abline(v=0.29, col="green")
abline(v=0.292874, col="red")
```
Die ROC-Kurve verläuft deutlich oberhalb der Diagonalen. Bei Anwendung des 
Modells wird somit eine höhere Vorhersagequalität erreicht, als mit reinem
Raten. Die grüne Linie zeigt den selbst geschätzten bzw. gesehenen Treshold am Ellenbogenpunkt. 
Die rote Line zeigt den berechneten Treshold.

## Auswahl Threshold aus ROC-Kurve ("Ellenbogenpunkt")
```{r}
threshold_glm_gesehen <- 0.29
threshold_glm <- 0.292874
```

## Modellgüte anhand der Konfusionsmatrix bestimmen und Threshholds vergleichen
```{r}
classify <- function(x, threshold){
  ifelse(x < threshold,0,1)
}
test_asdf$predict1 <- classify(test_asdf$prob, threshold = threshold_glm_gesehen)
test_asdf$predict2 <- classify(test_asdf$prob, threshold = threshold_glm_gesehen)

confusionMatrix(factor(test_asdf$predict1), factor(test_asdf$EmployeeChurned),
                positive = "1")
confusionMatrix(factor(test_asdf$predict2), factor(test_asdf$EmployeeChurned),
                positive = "1")
```
Der gesehene Ellnbogenpunkt und der berechnete threshhold liefern identische Ergebnisse auf dem Testdatensatz.

## Optimierung mit Kostenfunktion
```{r}
pred_class <- function(x, threshold){
  ifelse(x < threshold,0,1)
}

cost <- function(prob, actual,threshold2, cost_FN_func, cost_FP_func){
    predicted_automate2 <- pred_class(prob,threshold2)
    count_FN <- sum(predicted_automate2 == 0 & actual == "1")
    count_FP <- sum(predicted_automate2 == 1 & actual == "0")
    cost <- count_FN * cost_FN_func + count_FP * cost_FP_func
    return (cost)
}

threshold_input_rdf <- c(0.05, 0.1, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19,
                         0.2, 0.23, 0.24, 0.25, 0.29, 0.292874,
                         0.3, 0.35, 0.45, 0.7) 

i <- 1
costs_rdf <- c()
cost_FN <- 3000
cost_FP <- 500

while (i < length(threshold_input_rdf)+1) {
    costs_rdf[[(length(costs_rdf) +1)]] <- cost(test_asdf$prob, 
                                                test_asdf$EmployeeChurned, 
                                                threshold_input_rdf[i], cost_FN, cost_FP)
    i <- i+1
}

cost_rdf_matrix <- matrix(c(threshold_input_rdf,costs_rdf),nrow=length(threshold_input_rdf))
colnames(cost_rdf_matrix) <- c("Threshold", "Kosten in Euro")
cost_rdf_matrix

```
Mit Betrachtung der Kosten liegt der optimale Threshold für das Model bei 0,18.

## Modellgüte mit optimiertem Threshold anhand der Konfusionsmatrix bestimmen
```{r}
threshold_glm_opti <- 0.18
test_asdf$predict_opti <-classify(test_asdf$prob, threshold_glm_opti)
confusionMatrix(factor(test_asdf$predict_opti), factor(test_asdf$EmployeeChurned),
                positive = "1")
```
Verglichen mit vorher eingesetzten Thresholds ist dies der niedrigste, da eine 
FP-Klassifikation stärker bestraft wird, als eine FN-Klassifikation. Das bestätigt
auch die hohe Sensitivität.
Dadurch sinkt die Genauigkeit sowie die balanzierte Genauigkeit, wobei letztere
größer ist und verglichen zum Ausgangswert (F1-Score-optimierter Threshold) nur 
um 0,04 niedriger ist. 
Die 383 FP-Klassifizierten Werte sollten im Rahmen des Use-Cases kritisch betrachtet
werden, da diese Daten zur Unterstützung von Managern gelten soll und mti diesem
Ergebnis viel Arbeit für diese Nutzergruppe entstehen würde.

## Wichtigkeit der Features
```{r}
varimp_glm <- h2o.varimp(best_glm)
varimp_glm
```

