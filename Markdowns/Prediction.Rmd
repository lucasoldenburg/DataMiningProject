---
title: "Deskriptive Analyse"
output: html_document
---

## Employee Churn
# Prediction Martin

```{r}
library(readxl)
library(ggplot2)
library(tidyr)
library(stringr)
library(lubridate)
library(mltools)
library(data.table)
library(mlbench)
library(tidyverse)
library(magrittr)
library(caret)

#tree
library(h2o)
h2o.init()

```


## Datenimport und -vorbereitung
```{r}
hot_df <- read_xlsx("20220110_dataPrepped_afterOneHot.xlsx")
hot_df <- subset(hot_df, 
                  select = -c(EmployeeChurned_0, `start_Cost center`,
                              `start_Calendar Year/Month`,
                              `LastActive_Calendar Year/Month`))

colnames(hot_df)[colnames(hot_df) == "EmployeeChurned_1"] <- "EmployeeChurned"

# Kategorische Variablen
factorCols <- c("Gender",
                "TemporaryPersonel",
                "Part Time Mark",
                "NonExempt",
                "start_NonExempt",
                "development to exempt",
                "Vertical_Horizontal_Developments_>= 3",
                "Vertical_Horizontal_Developments_0",
                "Vertical_Horizontal_Developments_1-2",
                "start_contract type_apprentice",
                "start_contract type_permanent staff",
                "start_contract type_temporary personnel",
                "start_Part time mark",
                "EmployeeChurned",
                "decrease_Category_Customer",
                "decrease_Category_GB Management",
                "decrease_Category_GBR/N",
                "decrease_Category_GTU GD",
                "decrease_Category_Human Resources GBS",
                "decrease_Category_Intellectual Property",
                "decrease_Category_IT Service CC",
                "decrease_Category_People",
                "decrease_Category_Planning & Development",
                "decrease_Category_Procurement Services - GP",
                "decrease_Category_RAA/A Competence",
                "decrease_Category_Safety",
                "decrease_Category_Supplier & Reporting",
                "Nationality_Classification_EU",
                "Nationality_Classification_German",
                "Nationality_Classification_Other",
                "Paybands_A",
                "Paybands_B",
                "Paybands_C",
                "Paybands_D",
                "Paybands_E",
                "Paybands_F",
                "Paybands_G",
                "Paybands_H")

# Umwandlung in kategorische Variablen
hot_df %<>% mutate_at(factorCols, factor)

#Indexspalte
#hot_df$index <- 1:nrow(hot_df)
str(hot_df)
```

## Train Test Split (Caret)
```{r}
head(hot_df)
#stratified random split (caret)
inTrain <- createDataPartition(y=hot_df$EmployeeChurned, p = .75, list=FALSE)

trainC <- hot_df[ inTrain,]
testC <- hot_df[-inTrain,]

str(training)
str(testing)
```

## Train Test Split (sample)
```{r}
df <- data.frame(hot_df)
sortedDF <- sort(sample(nrow(df), nrow(df)*.75))
train<-df[sortedDF,]
test<-df[-sortedDF,]
```

## Train Test Split (h2o)
```{r}
hot_dfH2O <- as.h2o(hot_df)
splits <- h2o.splitFrame(data =  hot_dfH2O, ratios = 0.75, seed = 1234)
trainH2O <- splits[[1]]
testH2O <- splits[[2]]
```

# Find Features
```{r}
#https://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/model_selection.html

#automl
#https://docs.h2o.ai/h2o/latest-stable/h2o-docs/automl.html

#Grid Search
#Hyperparameter

#feature importance
#one hot or not?
```



# Einfacher Baum
```{r}
library(tree)

simpleTreeFit <- tree(training$EmployeeChurned ~ training$Gender + 
                        training$`Vertical_Horizontal_Developments_1-2`+
                        training$`decrease_Category_Procurement Services - GP`,
                        data = training)   

simpleTreeFit <- tree(EmployeeChurned ~ Gender + 
                        `Vertical_Horizontal_Developments_1-2`+
                        `decrease_Category_Procurement Services - GP`,
                        data = training) 
                        
                        #why?! Punkt funktioniert nicht... überall trainSample$... davor

simpleTreeFit <- tree(EmployeeChurned ~., data = train)

simpleTreeFit <- tree(EmployeeChurned ~., data = train)

plot(simpleTreeFit)
text(simpleTreeFit, pretty = 0)

```


#rpart initial
```{r}
library(rpart)
library(rpart.plot)

rpartInitial <- rpart(EmployeeChurned ~ Gender + 
                        `Month Of Service` +
                        `Months Since Last Development`+
                         `decrease_Category_Procurement Services - GP`,
                      data=training,
                      control= rpart.control(cp=0.01))

rpartAll <- rpart(EmployeeChurned ~.,
                  date=training,
                      control= rpart.control(cp=0.01))

rpartAll <- rpart(EmployeeChurned ~., data = hot_df2,
                      control= rpart.control(cp=0.01))

printcp(rpartInitial)
prp(rpartInitial)

printcp(rpartAll)
prp(rpartAll)
```


##rpart specifications
```{r}
rpartFit <- rpart(EmployeeChurned ~ . -index, data = training, method = "class",control = rpart.control(maxdepth = 5, minsplit = 5))
rpart.plot(rpartFit)
#
```


##h2o Distributed Random Forest (DRF)
Im DRF werden eine Reihe an Bäumen erzeugt. Jeder dieser lernt basierend auf einen Teil
des Datensets (Spalten und Reihen). Für die finale Vorhersage wird dann der Durchschnitt 
aus den Vorhersagen der einzelnen Bäume berechnet. Das verringert die Varianz.
Um robuste Bäume zu erhalten werden darüber hinaus auf jedem Level je Baum 
zufällig die betrachteten Spalten (Features) gewählt. Die Anzahl der verwendeten
Spalten wird dabei durch die Wurzel aus der Gesamtanzahl der Spalten definiert.

xxx Ausnahme wenn kein one hot encoding, dann 2

F1 Score für Split

```{r}
predictors <- c(factorCols, "Month Of Service", "Age", "Months Since Last Development")
target <- "EmployeeChurned"

h2oFit <- h2o.randomForest(x = predictors,
                             y = target,
                             ntrees = 10,
                             max_depth = 5,
                             min_rows = 10,
                             min_split_improvement = 0.0001,
                             calibrate_model = FALSE,
                             binomial_double_trees = TRUE,
                             training_frame = trainH2O,
                             nfolds = 6)
#Check: verbose: Print scoring history to the console. For DRF, metrics are per tree. 
#This option is defaults to false (not enabled).

# Güte im Trainingsset
perfH2O <- h2o.performance(h2oFit)
perfH2O
h2o.confusionMatrix(h2oFit)

# Anwendung auf Testset
predict <- h2o.predict(h2oFit, newdata = testH2O)
head(predict, 100)

# Güte anhand Testset
perfTestH2O <- h2o.performance(h2oFit, testH2O)
h2o.confusionMatrix(perfTestH2O)

curve_data <- data.frame(perfTestH2O@metrics$thresholds_and_metric_scores) %>% select(c(tpr,fpr))

ggplot(curve_data, aes(x = fpr, y = tpr)) +
    geom_point() +
    geom_line() +
    geom_segment(
        aes(x = 0, y = 0, xend = 1, yend = 1),
        linetype = "dotted",
        color = "grey40"
        ) +
    xlab("False Positive Rate") +
    ylab("True Positive Rate") +
    ggtitle("ROC Curve") +
    theme_bw()

h2o.gainsLift(h2oFit, testH2O)
h2o.auc(perfH2O)
h2o.aucpr(perfH2O)
h2o.accuracy(perfH2O)

#https://docs.h2o.ai/h2o/latest-stable/h2o-docs/performance-and-prediction.html?highlight=confusion%20matrix
```

##h2o Random Forest (Teilset)
```{r}

predictorsSmall <- c("Gender", "Month Of Service" ,
                "Months Since Last Development",
                "Part Time Mark", "Age",
                "Paybands_A", "Paybands_B", "Paybands_C", "Paybands_D", "Paybands_E")

targetSmall <- "EmployeeChurned"

h2oFitS <- h2o.randomForest(x = predictorsSmall,
                             y = targetSmall,
                             ntrees = 10,
                             max_depth = 5,
                             min_rows = 10,
                             min_split_improvement = 0.0001,
                             calibrate_model = FALSE,
                             binomial_double_trees = TRUE,
                             training_frame = trainingH2O,
                             nfolds = 6)


# Güte im Trainingsset
perfS <- h2o.performance(h2oFitS)
perfS
h2o.confusionMatrix(h2oFitS)

# Anwendung auf Testset
predictS <- h2o.predict(h2oFitS, newdata = testingH2O)
head(predictS, 100)

# Güte anhand Testset
perfTestS <- h2o.performance(h2oFitS, testingH2O)
h2o.confusionMatrix(perfTestS)

curve_dataS <- data.frame(perfTestS@metrics$thresholds_and_metric_scores) %>% select(c(tpr,fpr))

ggplot(curve_dataS, aes(x = fpr, y = tpr)) +
    geom_point() +
    geom_line() +
    geom_segment(
        aes(x = 0, y = 0, xend = 1, yend = 1),
        linetype = "dotted",
        color = "grey50"
        ) +
    xlab("False Positive Rate") +
    ylab("True Positive Rate") +
    ggtitle("ROC Curve") +
    theme_bw()
```

##h2o Random Forest und Grid Search (Tuning Hyperparameter)
```{r}
predictors <- c(factorCols, "Month Of Service", "Age", "Months Since Last Development")
target <- "EmployeeChurned"

#https://docs.h2o.ai/h2o/latest-stable/h2o-docs/grid-search.html#grid-search-in-r
#balance_classes = c(TRUE, FALSE)

drf_params <- list( balance_classes = c(TRUE, FALSE),
                    ntrees = c(30, 50, 70),
                    max_depth = c(10, 15, 20, 25),
                    min_split_improvement = c(0.0001, 0.00001, 0.000001))

#Hyperparameter
#balance classes=True or False (Oversample the minority classes to balance the class distribution. 
#This option is defaults to false (not enabled), and can increase the data frame size. 
#This option is only applicable for classification.)

#ntrees - default 50
#max_depth 20 default

#binomial_double_trees: (Binary classification only) Build twice as many trees (one per class). Enabling this option can lead to higher accuracy, while disabling can result in faster model building. This option is defaults to false (not enable
#Min_split_improvement


#search_criteria strategy = “Cartesian”

h2oGrid <- h2o.grid("drf", x = predictors, y = target,
            grid_id = "h2oGrid",
            training_frame = trainH2O,
            hyper_params = drf_params)

h2oGridPerf <- h2o.getGrid(grid_id = "h2oGrid",
                           sort_by = "auc",
                           decreasing = TRUE)

# Güte im Trainingsset
print(h2oGridPerf)
summary(h2oGridPerf)
best_drf <- h2o.getModel(h2oGridPerf@model_ids[[1]])


# Anwendung auf Testset
predictBestGrid <- h2o.predict(best_drf, newdata = testH2O)
head(predictBestGrid, 100)

# Güte anhand Testset
best_drf_perf <- h2o.performance(model = best_drf,
                                  newdata = testH2O)

best_drf_perf


curve_dataGrid <- data.frame(best_drf_perf@metrics$thresholds_and_metric_scores) %>% select(c(tpr,fpr))

best_drf_perf@metrics

ggplot(curve_data, aes(x = fpr, y = tpr)) +
    geom_point() +
    geom_line() +
    geom_segment(
        aes(x = 0, y = 0, xend = 1, yend = 1),
        linetype = "dotted",
        color = "grey50"
        ) +
    xlab("False Positive Rate") +
    ylab("True Positive Rate") +
    ggtitle("ROC Curve") +
    theme_bw()

```

##h2o Random Forest und Grid Search (Tuning Hyperparameter) ohne One Hot Encoding
```{r}



```



##Güte
Ziel unten links klein (false positve)


#h2o XGBoost or normal boost
```{r}
#nicht mit h2o verwendbar -> xgboost library von r
xgbFit <- h2o.xgboost(x = predictors,
                           y = response,
                           training_frame = trainingH2O,
                           booster = "dart",
                           normalize_type = "tree",
                           seed = 1234)



```


#caret train
```{r}
#test 
#hot_df$`Months Since Last Development`
treeFit <- train(EmployeeChurned ~ `Month Of Service`,
                     data = training,
                     method = "bstTree")
plot(treeFit)
text(treeFit, pretty =0)

#keine metrischen Werte
```
#caret predict
```{r}
treePredict <- predict(treeFit, newdata = testing)
#head(treePredict)
confusionMatrix(data = treePredict, testing$EmployeeChurned)
```


